{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68258c5a",
   "metadata": {},
   "source": [
    "# Portfolio Assignment week 02\n",
    "\n",
    "This week's focus is on manifold learning and text clustering. As part of the portfolio assignment, you are required to make a contribution to either the manifold learning case or the text clustering case. There are several options for your contribution, so you can choose the one that aligns with your learning style or interests the most\n",
    "\n",
    "## Text clustering\n",
    "\n",
    "Read, execute and analyse the code in the notebook tutorial_clustering_words. Then *choose one* of the assignments a), b) or c). \n",
    "\n",
    "c) Provide a text clustering solution with your own data of interest, you can follow a similar approach to the one in the tutorial_clustering_words notebook. \n",
    "\n",
    "Mind you that you are not allowed to copy code solutions without referencing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da01646",
   "metadata": {},
   "source": [
    "### A. Get and clean the text\n",
    "\n",
    "### Loading and getting data in the right form\n",
    "\n",
    "In this assignment I will be working on a text clustering solution for articles analysing different methods used for differential gene expression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d0b8179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anuk-k/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/anuk-k/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/anuk-k/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/anuk-k/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/anuk-k/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27e4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_text(url_list):\n",
    "    # Create empty list for storing article text\n",
    "    articles = []\n",
    "    \n",
    "    for url in url_list:\n",
    "        # Extracting html using given url\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Kill all script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        \n",
    "        # Extract article text and append to articles list\n",
    "        text = soup.get_text()\n",
    "        articles.append(text)\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "268ca15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Providing urls for brain expression related articles:\n",
    "article1 = {\"RNA-Seq differential expression analysis: An extended review and a software tool\":\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0190152\"}\n",
    "article2 = {\"Exaggerated false positives by popular differential expression methods when analyzing human population samples\": \"https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02648-4\"}\n",
    "article3 = {\"A Comparative Study of Techniques for Differential Expression Analysis on RNA-Seq Data\":\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0103207\"}\n",
    "article4 = {\"Comprehensive evaluation of differential gene expression analysis methods for RNA-seq data\":\"https://genomebiology.biomedcentral.com/articles/10.1186/gb-2013-14-9-r95\"}\n",
    "\n",
    "article_list = [article1,article2,article3,article4]\n",
    "url_list = []\n",
    "title_list = []\n",
    "for dict in article_list:\n",
    "    for key, value in dict.items():\n",
    "        title_list.append(key)\n",
    "        url_list.append(value)\n",
    "        \n",
    "articles = html_to_text(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290c65d",
   "metadata": {},
   "source": [
    "### Preprocessing and cleaning\n",
    "\n",
    "To cluster text, the text needs to be preprocessed with a regular natural language processer. In this step punctuation, stopwords or other unwanted text elements are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb05039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    Given a string from an article, removes new lines, capatilisation, punctuation, \n",
    "    references, and excess white space. Returns a string of cleaned text\n",
    "    '''\n",
    "    # removing new lines\n",
    "    text = re.sub('\\\\n', ' ', text)\n",
    "    # removing capitalised letters\n",
    "    text = text.lower()\n",
    "    # removing punctuation\n",
    "    text = re.sub('\\: |\\? |\\. |\\, |\\* |\\\\|\\: | \\- |\\;|\\:| \\| | +', ' ', text)\n",
    "    # removing references\n",
    "    text = re.sub('\\[\\d+ |\\,*\\d*\\]', ' ', text)\n",
    "    text = re.sub('\\(\\d+\\)', ' ', text)\n",
    "    # removing brackets\n",
    "    text = re.sub('\\(|\\)|\\[|\\]', '', text)\n",
    "    # removing excess space\n",
    "    text = re.sub('\\s{2,}|\\.{2,}', ' ', text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20688255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun extract and lemmatize function\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text \n",
    "    and pull out only the nouns.\n",
    "    Author: Fenna Feenstra\n",
    "    Date: 25-05-2023\n",
    "    Source: https://github.com/fenna/BFVM23DATASCNC5/blob/main/Tutorials/tutorial_clustering_words.ipynb\n",
    "    '''\n",
    "    # create mask to isolate words that are nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # store function to split string of words \n",
    "    # into a list of words (tokens)\n",
    "    tokenized = word_tokenize(text)\n",
    "    # store function to lemmatize each word\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # use list comprehension to lemmatize all words \n",
    "    # and create a list of all nouns\n",
    "    all_nouns = [wordnet_lemmatizer.lemmatize(word) \\\n",
    "    for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    \n",
    "    #return string of joined list of nouns\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca4cab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = []\n",
    "nouns_list = []\n",
    "\n",
    "for text in articles:\n",
    "    # Cleaning data and extracting nouns\n",
    "    clean = clean_text(text)\n",
    "    noun = nouns(clean)\n",
    "    # Storing in list\n",
    "    clean_list.append(clean)\n",
    "    nouns_list.append(noun)\n",
    "    \n",
    "# Store in dataframe\n",
    "df = pd.DataFrame(columns = ['title','text', 'noun'])\n",
    "df.title = title_list\n",
    "df.text = clean_list\n",
    "df.noun = nouns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63b214c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>noun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNA-Seq differential expression analysis: An e...</td>\n",
       "      <td>rna-seq differential expression analysis an e...</td>\n",
       "      <td>expression analysis review software tool skip ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exaggerated false positives by popular differe...</td>\n",
       "      <td>exaggerated false positives by popular differ...</td>\n",
       "      <td>positive expression method population sample b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Comparative Study of Techniques for Differen...</td>\n",
       "      <td>a comparative study of techniques for differe...</td>\n",
       "      <td>study technique expression analysis data skip ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comprehensive evaluation of differential gene ...</td>\n",
       "      <td>comprehensive evaluation of differential gene...</td>\n",
       "      <td>evaluation gene expression analysis method dat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  RNA-Seq differential expression analysis: An e...   \n",
       "1  Exaggerated false positives by popular differe...   \n",
       "2  A Comparative Study of Techniques for Differen...   \n",
       "3  Comprehensive evaluation of differential gene ...   \n",
       "\n",
       "                                                text  \\\n",
       "0   rna-seq differential expression analysis an e...   \n",
       "1   exaggerated false positives by popular differ...   \n",
       "2   a comparative study of techniques for differe...   \n",
       "3   comprehensive evaluation of differential gene...   \n",
       "\n",
       "                                                noun  \n",
       "0  expression analysis review software tool skip ...  \n",
       "1  positive expression method population sample b...  \n",
       "2  study technique expression analysis data skip ...  \n",
       "3  evaluation gene expression analysis method dat...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e8a20",
   "metadata": {},
   "source": [
    "### Part B: The Document-Term Matrix (DTM)\n",
    "\n",
    "To perform analyses we need to create a Document-Term Maxtrix. The Document-Term Matrix (DTM) represents the frequency of words (or terms) in a collection of documents. Each row in the matrix represents a document, and each column represents a word in the vocabulary. The value in each cell represents the frequency of the corresponding word in the corresponding document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc2e6225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.8, min_df=0.01,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a document-term matrix with only nouns\n",
    "# Store TF-IDF Vectorizer\n",
    "tv_noun = TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 1), max_df=0.8, min_df=0.01)\n",
    "# Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\n",
    "data_tv_noun = tv_noun.fit_transform(nouns_list)\n",
    "# Create data-frame of Doc-Term Matrix with nouns as column names\n",
    "data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=tv_noun.get_feature_names_out())\n",
    "data_dtm_noun.index = df.index\n",
    "# Visually inspect Document Term Matrix\n",
    "tv_noun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
