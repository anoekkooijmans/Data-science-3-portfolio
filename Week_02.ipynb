{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68258c5a",
   "metadata": {},
   "source": [
    "# Portfolio Assignment week 02\n",
    "\n",
    "This week's focus is on manifold learning and text clustering. As part of the portfolio assignment, you are required to make a contribution to either the manifold learning case or the text clustering case. There are several options for your contribution, so you can choose the one that aligns with your learning style or interests the most\n",
    "\n",
    "## Text clustering\n",
    "\n",
    "Read, execute and analyse the code in the notebook tutorial_clustering_words. Then *choose one* of the assignments a), b) or c). \n",
    "\n",
    "c) Provide a text clustering solution with your own data of interest, you can follow a similar approach to the one in the tutorial_clustering_words notebook. \n",
    "\n",
    "Mind you that you are not allowed to copy code solutions without referencing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da01646",
   "metadata": {},
   "source": [
    "### A. Get and clean the text\n",
    "\n",
    "### Loading and getting data in the right form\n",
    "\n",
    "In this assignment I will be working on a text clustering solution for articles analysing different methods used for differential gene expression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d0b8179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anuk-k/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/anuk-k/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/anuk-k/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/anuk-k/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/anuk-k/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88a8113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d27e4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_text(url_list):\n",
    "    # Create empty list for storing article text\n",
    "    articles = []\n",
    "    \n",
    "    for url in url_list:\n",
    "        # Extracting html using given url\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Kill all script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        \n",
    "        # Extract article text and append to articles list\n",
    "        text = soup.get_text()\n",
    "        articles.append(text)\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "268ca15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Providing urls for brain expression related articles:\n",
    "article1 = {\"RNA-Seq differential expression analysis: An extended review and a software tool\":\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0190152\"}\n",
    "article2 = {\"Exaggerated false positives by popular differential expression methods when analyzing human population samples\": \"https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02648-4\"}\n",
    "article3 = {\"A Comparative Study of Techniques for Differential Expression Analysis on RNA-Seq Data\":\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0103207\"}\n",
    "article4 = {\"Comprehensive evaluation of differential gene expression analysis methods for RNA-seq data\":\"https://genomebiology.biomedcentral.com/articles/10.1186/gb-2013-14-9-r95\"}\n",
    "\n",
    "article_list = [article1,article2,article3,article4]\n",
    "url_list = []\n",
    "title_list = []\n",
    "for dict in article_list:\n",
    "    for key, value in dict.items():\n",
    "        title_list.append(key)\n",
    "        url_list.append(value)\n",
    "        \n",
    "articles = html_to_text(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290c65d",
   "metadata": {},
   "source": [
    "### Preprocessing and cleaning\n",
    "\n",
    "To cluster text, the text needs to be preprocessed with a regular natural language processer. In this step punctuation, stopwords or other unwanted text elements are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbb05039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    Given a string from an article, removes new lines, capatilisation, punctuation, \n",
    "    references, and excess white space. Returns a string of cleaned text\n",
    "    '''\n",
    "    # removing new lines\n",
    "    text = re.sub('\\\\n', ' ', text)\n",
    "    # removing capitalised letters\n",
    "    text = text.lower()\n",
    "    # removing punctuation\n",
    "    text = re.sub('\\: |\\? |\\. |\\, |\\* |\\\\|\\: | \\- |\\;|\\:| \\| | +', ' ', text)\n",
    "    # removing references\n",
    "    text = re.sub('\\[\\d+ |\\,*\\d*\\]', ' ', text)\n",
    "    text = re.sub('\\(\\d+\\)', ' ', text)\n",
    "    # removing brackets\n",
    "    text = re.sub('\\(|\\)|\\[|\\]', '', text)\n",
    "    # removing excess space\n",
    "    text = re.sub('\\s{2,}|\\.{2,}', ' ', text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "20688255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun extract and lemmatize function\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text \n",
    "    and pull out only the nouns.\n",
    "    Author: Fenna Feenstra\n",
    "    Date: 25-05-2023\n",
    "    Source: https://github.com/fenna/BFVM23DATASCNC5/blob/main/Tutorials/tutorial_clustering_words.ipynb\n",
    "    '''\n",
    "    # create mask to isolate words that are nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # store function to split string of words \n",
    "    # into a list of words (tokens)\n",
    "    tokenized = word_tokenize(text)\n",
    "    # store function to lemmatize each word\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # use list comprehension to lemmatize all words \n",
    "    # and create a list of all nouns\n",
    "    all_nouns = [wordnet_lemmatizer.lemmatize(word) \\\n",
    "    for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    \n",
    "    #return string of joined list of nouns\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ca4cab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = []\n",
    "nouns_list = []\n",
    "\n",
    "for text in articles:\n",
    "    # Cleaning data and extracting nouns\n",
    "    clean = clean_text(text)\n",
    "    noun = nouns(clean)\n",
    "    # Storing in list\n",
    "    clean_list.append(clean)\n",
    "    nouns_list.append(noun)\n",
    "    \n",
    "# Store in dataframe\n",
    "data_nouns = pd.DataFrame(columns = ['title', 'text'])\n",
    "df.title = title_list\n",
    "df.text = nouns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d74019a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>noun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNA-Seq differential expression analysis: An e...</td>\n",
       "      <td>expression analysis review software tool skip ...</td>\n",
       "      <td>expression analysis review software tool skip ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exaggerated false positives by popular differe...</td>\n",
       "      <td>positive expression method population sample b...</td>\n",
       "      <td>positive expression method population sample b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Comparative Study of Techniques for Differen...</td>\n",
       "      <td>study technique expression analysis data skip ...</td>\n",
       "      <td>study technique expression analysis data skip ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comprehensive evaluation of differential gene ...</td>\n",
       "      <td>evaluation gene expression analysis method dat...</td>\n",
       "      <td>evaluation gene expression analysis method dat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  RNA-Seq differential expression analysis: An e...   \n",
       "1  Exaggerated false positives by popular differe...   \n",
       "2  A Comparative Study of Techniques for Differen...   \n",
       "3  Comprehensive evaluation of differential gene ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  expression analysis review software tool skip ...   \n",
       "1  positive expression method population sample b...   \n",
       "2  study technique expression analysis data skip ...   \n",
       "3  evaluation gene expression analysis method dat...   \n",
       "\n",
       "                                                noun  \n",
       "0  expression analysis review software tool skip ...  \n",
       "1  positive expression method population sample b...  \n",
       "2  study technique expression analysis data skip ...  \n",
       "3  evaluation gene expression analysis method dat...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e8a20",
   "metadata": {},
   "source": [
    "### Part B: The Document-Term Matrix (DTM)\n",
    "\n",
    "To perform analyses we need to create a Document-Term Maxtrix. The Document-Term Matrix (DTM) represents the frequency of words (or terms) in a collection of documents. Each row in the matrix represents a document, and each column represents a word in the vocabulary. The value in each cell represents the frequency of the corresponding word in the corresponding document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc2e6225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/anuk-k/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'ENGLISH_STOP_WORDS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6320/3902533754.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create a document-term matrix with only nouns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Store TF-IDF Vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtv_noun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'ENGLISH_STOP_WORDS'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Create a document-term matrix with only nouns\n",
    "# Store TF-IDF Vectorizer\n",
    "tv_noun = TfidfVectorizer(stop_words=text.ENGLISH_STOP_WORDS, ngram_range = (1,1), max_df = .8, min_df = .01)\n",
    "\n",
    "# Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\n",
    "#data_tv_noun = tv_noun.fit_transform(data_nouns.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bdf8449c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6320/1620908231.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit and transform speech noun text to a TF-IDF Doc-Term Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_tv_noun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtv_noun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_nouns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2075\u001b[0m         \"\"\"\n\u001b[1;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1221\u001b[0m                     \u001b[0;34m\"empty vocabulary; perhaps the documents only contain stop words\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
